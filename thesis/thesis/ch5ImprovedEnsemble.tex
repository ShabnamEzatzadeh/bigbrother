\chapter{ACTIVITY RECOGNITION ENSEMBLE FORECASTING}
In the past chapter, we showed that a few changes can lead to some significant improvements in multi-step forecasting accuracy when ensemble Bayesian combined forecasting is applied to our traffic datasets.  This chapter initially investigates BCF further by exposing some potential problem scenarios.  We look at some related work into potentially solving these problems through the identification and modeling of activities and finally introduce a new ensemble forecasting technique which solves many of these problems.  

\begin{figure}[!b]
	\begin{center}
		\subfigure[] {
			\includegraphics[width=0.49\textwidth]{sample_residual_event_804.png}
		}
		\subfigure[] {
			\includegraphics[width=0.49\textwidth]{sample_residual_event_6024.png}
		}
	\end{center}
	\caption{Two similar events occurring at different time in the same Denver residual dataset.}
	\label{fig:sample_residual_events}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Need for another ensemble forecaster
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The need for another ensemble forecaster}
When analyzing the residual datasets of these ensemble forecasts, we noticed a fundamental problem with our forecaster.  In the residual forecasts of BCF and many of the forecasting algorithms, the resulting residual forecast would still have many repeated misforecasts that could not be explained by random noise.  As alluded to in Chapter 1, these repetitive misforecasts may come from large human controlled scheduled events (such as sporting events, public celebrations, or in the case of buildings - meetings) or they may be from uncontrolled and unscheduled events (such as weather or traffic accidents).  

\begin{figure}[!t]
	\begin{center}
		\subfigure[] {
			\includegraphics[width=0.49\textwidth]{denver_bcf_residual_15.png}
		}
		\subfigure[] {
			\includegraphics[width=0.49\textwidth]{denver_bcf_residual_26.png}
		} \\
		\subfigure[] {
			\includegraphics[width=0.49\textwidth]{denver_bcf_residual_32.png}
		}
		\subfigure[] {
			\includegraphics[width=0.49\textwidth]{denver_bcf_residual_38.png}
		}
	\end{center}
	\caption{Scaled histogram of BCF-TS residual values for the Denver traffic dataset at various daily time steps.  The red line is the corresponding best fit Normal distribution.  Notice how in all plots there exist data points which exist after the tails of the Normal distributions have approached zero.}
	\label{fig:denver_bcf_residual}
\end{figure}

\ref{fig:sample_residual_events} shows two of these events occurring.  Such an event clearly occurs outside the normal behavior noise of our data.  The light red region in this image represents the one standard deviation boundary for the residual data.  From Chapter 4, we know that BCF residual data tends to be normally distributed per daily time step.  The residual datasets from each time step pass the one-sample Kolmogorov-Smirnov \cite{Marsaglia2003, Lopes2007} test for normality ($p \ge 0.1$).  Assuming this data is truly normally distributed then the odds of getting even one such datapoint outside the $\pm 3 \sigma$ should occur roughly 1 in every million data points.  Yet, in our Denver traffic data, we had 22 such instances in the testing dataset alone ($N \approx 12000$).  

\ref{fig:denver_bcf_residual} the distribution of our datasets graphically.  In this figure, a histogram of the data from four different fixed daily time steps from the Denver traffic dataset, overlaid with the corresponding best fit normal distribution.  In each image the same pattern appears.  Residual values around the mean and on the tails tend to occur with a greater frequency than would be suggested by a normal distribution.  Values around the mean are not the problem.  These values correspond to accurate forecasts.  Values around the tail however are problematic and are further evidence of large events being one of the causes of noise in our residual data.

The unlikely existence of these tail events combined with evidence that the overall residual time series behaves as a white noise Gaussian process provides evidence that these events are likely not due to natural noise and instead due to other factors.  The remainder of this chapter discusses ways to identify and model these anomalous events along with providing an algorithm for using these events to improve our forecasting algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Overview ABCF
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian combined forecasting with activity and anomaly modeling (ABCF)}

\begin{wrapfigure}{L}{0.3\textwidth}
\centering
\includegraphics[width=0.25\textwidth]{simple_approach_overview.png}
\caption{Extremely high level overview of our approach}
\label{fig:highlevel_overview}
\end{wrapfigure}

As outlined in the introduction, we return here to a brief discussion of our approach.  The structure of non-Gaussian anomalies in the residual datasets of our forecasts leads us to explore if models of these anomalies can be used to improve our forecasts.  We propose a hybrid ensemble approach which can be applied to any forecaster and may lead to improved results.  

A high level view of our approach is detailed in \ref{fig:highlevel_overview}.  Given any trained forecaster, we extract a set of residual data.  Searching this residual dataset, we next attempt to extract and model the most representative set of anomalous events.  From \ref{fig:sample_residual_events}, we know that for our traffic systems - and we believe for many other traffic systems with repeated events - these events are not unique and are likely to repeat with similar behavior.  These anomalous events are then used as the basis of an ensemble forecaster combined with a zero mean background model which represents the native forecaster.  

In Section \ref{sec:abcf} we introduce a recursive ensemble forecaster which uses clustered anomalies as inputs and works on the residual dataset of any given native forecaster.  Finally, using both the anomaly ensemble inputs and the native forecast, we combine the results to produce a final forecast.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Background literature
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background literature on activity recognition and anomaly detection}
To construct our approach we need to identify and model these anomalous events.  Considerable work has been done in the field of time series anomaly detection; however this work is typically limited to anomaly detection and classification.  We were unable to find many academic references using anomaly modeling techniques for improved forecasting.  We believe that this limited amount of anomaly modeling work is due to the nature of anomalies; either they are random and difficult to predict as is the case with stock market anomalies ~\cite{Kontonikas2013, Thushara2014} or they require some immediate attention and thus forecasting is an inappropriate response to the anomaly.  Such a situation arises frequently in network data monitoring.  Detection and classification of network anomalies help to determine potential network attacks \cite{Tartakovsky2013, Gogoi2011} and allow for preventive measures to take place, but forecasting network traffic during these anomalies is of little utility.  
   
Our anomalies tend to be repetitive and multiple time steps in length.  Data of this form is very closely related to another field of time series data analysis - that of activity recognition.   Due to the broad nature of this term and the breadth of research on anomaly detection, we briefly discuss some of the ways in which time series anomaly detection and activity recognition has been utilized in the past as a way to familiarize the reader with a discussion of the literature.  This review is not meant to be a complete list of all works in these fields, but instead gives an overview of the types of work done in this field so that we may better contrast previous work with our approach.

\bigskip
\noindent \textbf{Anomaly Detection} \\
From Eamon Keogh, a prominent researcher in time-series anomaly detection, a reading or series of readings is anomalous in a time-series if the 

\begin{quote}
"frequency of occurrences differed substantially from that expected, given previously seen data. \cite{Keogh2002}"
\end{quote}

A common method of anomaly detection is through the use of tools to assist in visual identification \cite{Stoffel2013, Lakhina2004, Shi2012}.  Tools have been extensively developed for network anomaly detection.  Such tools allow network administrators and researchers to quickly identify and potentially classify anomalies in the form of certain network attacks.  This is done by giving providing graphs, and overlay visualizations which make the identification of patterns more apparent.  Through the use of these tools, network administrators are able to quickly respond to  various attacks and minimize the potential damage to the network.  Visual assistance tools for anomaly detection do not provide modeling and forecasting of anomalies and thus are of little utility for utilizing anomalies to improve traffic system forecasting.   

Another common technique for anomaly detection within time series data is known as change point (or step detection).  Originally developed for statistical quality control, the cumulative sum control chart (CUSUM) \cite{page1955} is a classic algorithm for detecting changes in the mean of a time series.  It involves the calculation of a cumulative sum of the weighted observations.  When this sum exceeds a certain threshold value, a change in value is declared. 

The field of change point detection is quite heavily researched.  Researchers have developed algorithms for most types of data and computational scenarios.  There are algorithms to detect changes in time series mean, changes in variance and changes in distribution from the exponential family \cite{Dessein2013}.  These algorithms can operate offline, online \cite{Tartakovsky2013}, top down, bottom up and globally.  Excellent summaries of current state of the art in change point detection spanning all major formulations (Bayesian, minimax, and generalized Bayesian) are found in \cite{Polunchenko2012, Reeves2007}.  

For some time after the original CUSUM change point method, research moved to more parametric stochastic models.  Researchers would look for statistical changes in a time series that fit another distribution or family of distributions, for example Adams and MacKay's paper on Bayesian Online Change Point Detection \cite{Adams2007}.  Such methods are powerful when the change point distributions are known a priori, however we do not assume such knowledge.  

Another more recent paper from Liu et al. \cite{Liu2013} discusses an entirely different change point detection technique which relies on the ratio between the distribution of some samples at time $t$ and some further samples at time $t + n$.  Through ratio estimation, researchers can use a dissimilarity measurement on non parametric models of the data to estimate change points.  The power of this technique comes from directly estimation the ratio of probability densities and not the densities themselves.  From this density estimation, the rational is that knowledge of the two densities implies the ratio, but the ratio does not uniquely imply the densities and thus the ratio may be easier to estimate while still giving knowledge of the change point.  

The problem with general change point techniques for our data is that our data distributions change constantly throughout the day.  Our MERL dataset shows large amounts of activity during lunch time and work start and end times.  We are interested in those instances where activities are anomalous.  Our technique uses a combination of peak finding and time of day residual CUSUM.  A detailed description of our technique and results will be shown later.

\bigskip
\noindent \textbf{Individual Activity Modeling and Recognition} \\
Work in activity recognition has focused on recognizing either individual activities or group activities.  In this section we describe many of the individual activity recognition techniques.  One common type of individual activity recognition is from wearable sensors such as accelerometers or RFID tag readers.  This type of work is almost always supervised and the goal is to map sensor readings to a comprehensible activity such as dish washing or tooth brushing \cite{Wang2009,Bao2004}.  While some of this has potential applications to our goals, much of it is not applicable as the focus is typically on recognizing activities from fully labeled datasets.  Authors from this field have used many of the standard machine learning models: decision trees \cite{Bao2004}, support vector machines \cite{Krishnan2008,Bao2004,Lustrek2009}, naive Bayes \cite{Bao2004,Lustrek2009}, nearest neighbor \cite{Bao2004,Lustrek2009}, and hidden Markov Models (HMM) \cite{Wang2009,Oliver2002}.  Comparisons amongst models have shown that performance is data dependent and that no one model appears to be best for all types of activities \cite{Bao2004,Lustrek2009}

Huynh \cite{Huynh2008} used a naive Bayes classifier in a different way for wearable sensor individual activity recognition.  Instead of using it to describe activity, it was used as a dimensionality reduction technique; the results of which were the basis for a dictionary in latent Dirichlet allocation \cite{Blei2003}.  The topics generated from latent Dirichlet allocation are then clustered using k-means.  Each of these clusters represents a single activity.  This clustering approach proved effective for the recognition of repeated activities throughout the day, but due to its reliance of a fixed ratio of latent Dirichlet allocation projected topics, it is likely that recognizing combinations of activities will prove problematic.

To account for activities of varying time lengths, probabilistic suffix trees \cite{Hamid2007} have shown to be an effective model for activities.  Trees are trained using all sequential subsets of an input sequence and a total model is then created from the set of trees using AdaBoost \cite{Freund1996}.  The performance level of suffix trees seems to be highly noise dependent.  \cite{Hamid2006} compared HMMs with suffix trees and found that suffix trees out performed HMMs when the data was without noise, but as the noise increased HMMs performed increasingly better, eventually surpassing the performance of suffix trees.

\bigskip
\noindent \textbf{Group Activity Modeling and Recognition} \\
There are a limited number of publications that exist on group activity modeling recognition using a large number of sensors.  Within this problem domain the challenges to solve are different due to the type of data collected and due to group activities typically occurring over multiple sensors.  The work within this domain that is most similar to our work is from Mitsubishi Electronic Research Laboratories (the creators of the MERL dataset) \cite{Wren2003, Wren2006a}.  The goal of such group activity work is to utilize a set of sensors to describe activities or events which effect many individuals within the environment.  As expected, these group event models are typically used to describe larger or longer movements within the environment.  For example, such modeled events may be holiday movement vs workday movement or general flow through out a building based on sensor topology \cite{Wren2006, Hoff2009}.  
	
HMMs have been used as a model for learned activities.  These models are used to build a tree \cite{Minnen2004, Wren2006a} with each level described by a model with a different number of hidden nodes, meaning that model accuracy is roughly correlated to tree depth.  At the top of the tree are simple models used to describe gross activities.  The leaves of the tree are highly complex models describing specific activities.  This tree structure has the advantage of being computationally efficient while maintaining accuracy on par with other techniques based on clustering of HMMs \cite{Clarkson1999}.  
	
In a method similar to the HMM tree, work has been done to create a hierarchy of fixed filters based on possible sensor topologies \cite{Wren2006}.  At each level of the hierarchy, the number of sensors and the amount of time history increases.  The probability of occurrence of each fixed arrangement is then computed when all levels are created, the resulting model represents the total classifier.  The usage of fixed sensor topologies is highly environment dependent and the fixed time lengths with each level of the hierarchy are likely too restraining for the types of activities we expect to observe.  

From the results of all activity recognition papers, it appears that approaches which show the most promise tend to use a model which allows comparison of inputs with various time lengths.  Also, hierarchical techniques tend to perform better than multiple models of equal complexity.  In defense of these general observations is the work of Huynh \cite{Huynh2005} who found that empirically for his problem, there is not a single feature or time window of past history that will perform best for all activities.  Instead each activity is best modeled by a set of features and length of time unique to that activity.  Huynh postulates that his finding is true of most activity recognition problems.  Our approach, which will be described in detail in the next section is to represent activities as a mixture of Gaussians.  While for this work we use fixed length activities, our ensemble forecaster (described in Section 5.7) is derived to work with activities each which may have any length independent of the other modeled activities.  Modeling mixed length activities is recommended for future work.

\subsection{Anomaly extraction and representation}
As we showed earlier in this section with ~\ref{fig:denver_bcf_residual}, the tails of the best fit Gaussian for the BCF residual random variable tend to have larger than expected likelihoods.  These unlikely residuals do not appear to be completely random.  From a review of the literature, we know that there are numerous methods to extract and model this data.  For our work, we propose a simple extraction technique and then model the data according to a time-series mixture of Gaussians.  This method allows us to impart a probabilistic behavior to our anomaly models.  Such behavior is essential to our eventual forecasting technique.

\begin{figure}[]
	\begin{center}
		\subfigure[] {
			\includegraphics[width=0.50\textwidth]{slide_window_1.png}
		}
		\subfigure[] {
			\includegraphics[width=0.50\textwidth]{slide_window_2.png}
		}
	\end{center}
	\caption{Demonstration of a fixed width sliding window looking for locally maximal deviations from background behavior.}
	\label{fig:sliding_window}
\end{figure}

\bigskip
\noindent \textbf{Sliding window data extraction} \\
Given a time series of residual data, we extract the top $k\%$ of the maximum residual data as measured by a fixed length sliding window.  ~\ref{fig:sliding_window} demonstrates visually our method of potential candidate residual data segments to model.  In this example, the fixed length window is slid along a time series and windows with large total residual deviation are selected.  These windows are then centered around the largest peak within the window.  This is done to ensure an easier time clustering as similar extracted residual segments will typically line up with each other.

\begin{algorithm}
	\caption{Algorithm for candidate data extraction}
   	\label{alg:dataextract}
	
	\begin{algorithmic}
		\State \#assume we have access to residual\_data as a one dimensional array
		\State 
		\State extracted\_lengths = []
		\State final\_data = []
   		\State 
				
		\For{$t = 0$; $t <=$ residual\_data.length - window\_width; $t +=$ window\_width}
		\State	window\_sum = sum(abs(residual\_data[$t$:$t +$ window\_width]))
		\State 	extracted\_lengths.append((window\_sum, $t$))
		\EndFor
		
		\State
		\State \#sort the extracted lengths by the window\_sum dimension descending
		\State extracted\_lengths = sort(extracted\_lengths, dim1, order = "descending")
		\State num\_to\_extract = ceil($k$ * extracted\_lengths.length)
		
		\State
		\State \#Center all the extracted residuals by local peaks
		
		\For {$t = 0$; $t < num\_to\_extract$; $t++$}
		\State 	dind = extracted\_lengths[t][1]
		\State	\#Max peak on local window
		\State	mp = find\_max\_peak(abs(residual\_data[dind:dind + window\_width]))
		
		\State
		\State 	\#Recenter the data around the local peak
		\State	final\_data.append(residual\_data[mp - window\_width / 2: mp + window\_width / 2])

		\EndFor

	\end{algorithmic}
\end{algorithm}


Algorithm \ref{alg:dataextract} precisely describes our approach.  The input parameter $k$ is chosen empirically for each dataset and underlying forecasting model.  The effects of this parameter $k$ are discussed later.  A sample of the top 10\% extracted residuals for the MERL dataset is displayed in \ref{fig:extracted_residuals}.

On additional possible technique for data extraction is sparse dictionary coding.  This technique finds a set of primitives (in this case small representations of the time series residual data) which can be used to best approximate the original dataset.  We believe it likely that primitives with the largest residual values would most likely correspond to the anomalies we would like to find.  This technique has been extensively to find primitives activations on time series EMG data \cite{Kim2010} and for image reconstructions \cite{Mairal2009}.  

For this work however, we believe that our constant length anomaly extraction is sufficient to show the strength of our combined forecasting approach.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.90\textwidth]{arima_abcf_extracted_residuals_Merl.png}
	\end{center}
	\caption{Extracted residuals from the MERL dataset using the sliding window extraction method with window length of 7}
	\label{fig:extracted_residuals}
\end{figure}


\subsection{Time series mixture of Gaussians derivation}
A mixture of Gaussians is a strongly supported stochastic data clustering technique used in activity recognition.  This technique models a set of data by positing that each data point ($x_{i}$) is generated from one of $K$ Gaussians.  Borrowing partially the notation of Andrew Ng \cite{Ng2008}, a mixture of Gaussians is described mathematically by specifying a joint distribution $p(x_{i}, z_{i}) = p(x_{i}|z_{i})p(z_{i})$.  $z_{i}$ is multinomial on parameter $\phi$  ($\phi_{j} \ge 0$,  $\sum_{j=1}^{K}{ \phi_{j}} = 1$ and $p(z_{i} = j)$).  Each $x_{i}|z_{i} = j \sim \mathcal{N}(\mu_{j}, \Sigma_{j})$.  From this derivation, the $z_{i}$'s are latent variables which makes the estimation of the Gaussian models more difficult.  The result of mixture of Gaussian training is to produce a soft clustering of every datapoint $x_{i}$ to each trained Gaussian distribution.

Traditionally, a mixture of Gaussians is for data vectors with no time element, but some research has been done on using a mixture of Gaussians for time series data clustering.  In one paper \cite{Eirola2013}, the authors create a design matrix to construct a delayed embedding of the original time series.  Each row in the matrix corresponds to a fixed length vector selected from the input time series.  All of the rows together contain all data from the time series with each row over containing overlapping data with one new time series element.  From this embedding, the authors compute a mixture of Gaussians to approximately reconstruct the time series.  In another paper \cite{Kalliovirta2012} Gaussian mixture models are used to mix the weights of an autoregressive model in an attempt to reconstruct the original time series.

Here we introduce another technique for modeling time series data were we assume the presence of multiple short (typically fewer than 10 time steps) residual time series.  These time series are assumed to be derived from a set of $K$ unique time series.  In each of these time series, we model the time steps as Gaussians, each with their own unique mean and covariance.   For our work, the input time series are those candidate extracted anomalies from the previous section (i.e. we are attempting to model and cluster data from \ref{fig:extracted_residuals}).  The following is our derivation of this approach.

The goal of mixture of Gaussians is to find a set of models which will maximize the log likelihood of the parameters of some models to the dataset.  Given dataset (in this case a dataset of residuals) $\{r^{(i)}\}$ we maximize
\begin{equation}
\ell(\phi, \mu, \Sigma) = \sum_{i = 1}^{M}log\{p(r^{(i)};\phi, \mu, \Sigma)\}
\end{equation}
\noindent 
where ${M}$ is the total number of time series instances.  In this equation, the $r^{(i)}$'s are each time series slices from the candidate residuals.  The $(i)$ represents the $i$th residual candidate time series - we use the $(i)$ notation to distinguish from the exponent operator.  For this work, each $r^{(i)}$ is of fixed length $N$.  $\phi, \mu,$ and $\Sigma$ are the parameters to optimize for each time series Gaussian.  Each $r^{(i)}$ is roughly analogous to $x_{i}$ in the typical derivation of a mixture of gaussians with the difference here that each $r^{(i)}$ is $N$ readings in length.

As in traditional mixture of Gaussians formulation, we introduce a latent variable $z^{(i)}$ which is multinomial on parameter $\phi$ ($\phi_{j} \ge 0$,  $\sum_{j=1}^{K}{ \phi_{j}} = 1$ and $p(z_{i} = j)$).  With this latent variable $z^{(i)}$ we now maximize the likelihood of the joint distribution of $r$ and $z$.  This equation is given as 

\begin{equation}
\ell(\phi, \mu, \Sigma) = \sum_{i = 1}^{M}\log \sum_{k = 1}^{K}p(r^{(i)}, z^{(i)} = k;\phi, \mu, \Sigma)
\end{equation}

In the traditional mixture of Gaussians algorithm each model is ostensibly a Gaussian which may be multivariate on $x^{(i)}$.  One option to model time series data using a mixture of Gaussians is to use each residual input window as multi dimensional training data.  A set of K multi variate Gaussians would then be used to fit the training set.  This type of model does not work with our activity time series forecaster introduced later.  To make this algorithm work with time series data, we define the models instead by
\begin{equation}
\label{eq:model}
p(r^{(i)}|z^{(i)} = k;\mu, \Sigma) = \prod_{n = 1}^{N}p(r^{(i)}_{n};\mu_{n}, \Sigma_{n}).
\end{equation}
\noindent
In this equation, $N$ is the length of each time series instance.  Each $p(r^{(i)}_{n};\mu_{n}, \Sigma_{n})$ is computed by a different Gaussian at time offset $n$.  Thus our model for each time series is $N$ independent Gaussians.

Finally, to make our notation a bit easier we define a variable $w^{(i)}_{k}$.  
\begin{equation}
w_{k}^{(i)} = p(z^{(i)} = k|r^{(i)};\mu, \Sigma)
\end{equation}
\noindent
$w_{k}^{(i)}$ represents the probability of latent variable $z^{(i)}$ taking the value $j$ from our set of models defined in equation \ref{eq:model}

Expectation maximization (EM) is a common technique to solve maximization problems of this kind.  Here we briefly derive the EM algorithm for our time series derivation of mixture of Gaussians.  Combining our equations so far, we can write the likelihood of the time series model as
\begin{equation}
\label{eq:pre_em}
\ell(\phi, \mu, \Sigma) = \sum_{i = 1}^{M}\log \sum_{k = 1}^{K}w_{k}^{(i)}\frac{p(r^{(i)}, z^{(i)}=k;\phi, \mu, \Sigma)}{w_{k}^{(i)}}
\end{equation}
\noindent
This is the equation we want to maximize.  The EM algorithm is computed in two steps, the E step which gives a lower bound on the equation we are trying to maximize and then the M step which maximizes for the current lower bound.  

\bigskip
\noindent
\textbf{E-Step} \\
The E-step hardly changes from the traditional EM mixture of Gaussians algorithm.  We simply need to calculate 
\begin{equation}
w^{(i)}_{k} = p(z^{(i)} = k|r^{(i)}).
\end{equation}
\noindent
This can be calculated directly from the data by applying Bayes rule.

\begin{equation}
w^{(i)}_{k} = p(z^{(i)} = k|r^{(i)}) = \frac{p(r^{(i)}|z^{(i)}=k)p(z^{(i)}=k)}
						{\sum_{k=1}^{K}p(r^{(i)}|z^{(i)}=k)p(z^{(i)}=k)}
\end{equation}


\bigskip
\noindent
\textbf{M-Step} \\
For the maximization step it is assumed that we know the values of $w_{k}^{(i)}$.  Thus, we need to maximize equation \ref{eq:pre_em} with respect to $\mu$, $\Sigma$, and $\phi$.  However there is one more simplifying step which needs to be made prior to direct maximization.  This can be made according to Jensen's inequality, that is equation \ref{eq:pre_em} may be written as 
\begin{equation}
\label{eq:em_likelihood}
\ell(\phi, \mu, \Sigma) = \sum_{i = 1}^{M}\sum_{k = 1}^{K}w_{k}^{(i)}\log \frac{p(r^{(i)}|z^{(i)}=k;\mu, \Sigma)p(z^{(i)} = k;\phi)}{w_{k}^{(i)}}
\end{equation}
and still converge using the EM algorithm.

Combining equations~\ref{eq:em_likelihood} and~\ref{eq:model} gives the following log likelihood
\begin{equation}
\label{eq:em_combined}
\ell(\phi, \mu, \Sigma) = \sum_{i = 1}^{M}\sum_{k = 1}^{K}w_{k}^{(i)} [\log \frac{p(z^{(i)} = k)}{w_{k}} + \sum_{n = 1}^{N} \log \frac{p(r^{(i)}; \mu_{k, n}, \Sigma_{k, n})}{w_{k}}]
\end{equation}

Maximizing \ref{eq:em_combined} with respect to $\phi_{k}$, $\mu_{k, n}$ and $\Sigma_{k, n}$ yields the following update rules for each parameter in the M step.

\begin{equation}
\phi_{k} = \frac{1}{M}\sum_{i = 1}^{M}w_{k}^{(i)}
\end{equation}
\begin{equation}
\mu_{k, n} = \frac{\sum_{i = 1}^{M}w_{k}^{(i)}r^{(i)}_{n}}{\sum_{i = 1}^{M}w_{k}^{(i)}}
\end{equation}
\begin{equation}
\Sigma_{k, n} = \frac{\sum_{i = 1}^{M}w_{k}^{(i)}(r^{(i)} - \mu_{k, n})(r^{(i)} - \mu_{k, n})^{\mathrm{T}}}{\sum_{i = 1}^{M}w_{k}^{(i)}}
\end{equation}


\subsection{Selecting the number of clusters}

Determining an appropriate number of clusters is an important task in cluster analysis.  Because of its importance for any clustering technique, the literature on this subject is extensive.  While there is no universal definition for determining the optimal number of clusters, there is a general similarity for most cluster analysis techniques.  That is, grouping objects (activity residuals for our dataset) which are as similar as possible into sets, while keeping the model representing the clusters as dissimilar as possible.    A full discussion of all of these approaches is too broad for our work.  Here we will discuss a few common approaches and describe the approach we take to clustering.

\bigskip
\noindent
\textbf{Elbow}

\begin{wrapfigure}{R}{0.4\textwidth}
\centering
\includegraphics[width=0.4\textwidth]{elbow_clusters.png}
\caption{Demonstration of the elbow clustering selection [ref].}
\label{fig:elbow_cluster}
\end{wrapfigure}

Developed by Robert Thorndike \cite{Thorndike1953} in 1953, this approach looks to plot the inter-cluster sum of squared error verses the number of clusters.  In many clustering applications, the average inter-cluster error will begin to drop significantly and then begin to asymptotically decrease.  The number of clusters is chosen at this "elbow."  \ref{fig:elbow_cluster} demonstrates a this elbow.  In the image, the number of clusters selected is 2.  A common problem with this approach is that the elbow can not always be unambiguously identified.

\bigskip
\noindent
\textbf{Information criterion} \\
An information based approach to selecting the number of clusters is to use either the Akaike information criterion \cite{Akaike1974} or the Bayesian information criterion \cite{Schwarz1978}.  There exist other information criterions, but they do not have the ubiquity of the prior two criterions.  Both the Akaike and the Bayesian criterion seek to determine the number of clusters by imposing a tradeoff between the goodness of fit and the number of model parameters.  For our work, goodness of fit can be measured by the log likelihood of the data to the Gaussian clusters and the number of model parameters is simply the number of models.  

\bigskip
\noindent
\textbf{Cross-validation} \\
TODO WRITE ON CROSS-VALIDATION METHOD


\bigskip
\noindent
\textbf{Silhouette} \\
Silhouette \cite{Rousseuw1987} scores are another method of selecting the number of clusters.  A silhouette score balances the tradeoff between the average dissimilarity between all data points for a given cluster and the average dissimilarity to the data points of the next nearest cluster.  The silhouette score is defined as

\begin{equation}
s(i) = \frac{b(i) - a(i)}{max\{a(i), b(i)\}}.
\end{equation}

$a(i)$ is the average dissimilarity between data point $i$ and all other points within the cluster $i$ is assigned.  $b(i)$ is the average dissimilarity between data point $i$ and all other points within the nearest cluster that does not contain $i$.  The optimal number of clusters is then chosen by finding the maximum silhouette score amongst multiple possible values of K.  Because mixture of Gaussians is a soft cluster assignment algorithm (each point has a likelihood to be assigned to each cluster instead of belonging to one single cluster), we consider the assignment of each point to be only the Gaussian from which it is most likely modeled.

The best method for selecting the optimal number of clusters is an open problem.  Different datasets and different clustering methods may require different criterions to determine the optimal number of clusters.  Since our approach is mostly unsupervised, we do not have the luxury of determining what criterion works best for our data.   

The doctoral thesis \cite{Yan2005} of Yan broached the topic of best clustering criterion and showed significant variation between clustering criterions.  Of these criterions, silhouette based cluster selection was consistently one of the best selection criterions.  Given the variability in selection techniques and combined with Yan's findings, we feel confident that Silhouette scores provide an adequate cluster selection criterion.

\subsection{Sample representative clusters and a discussion about them}

This section visually demonstrates a few of the clusters extracted from our datasets using the time series mixture of Gaussian's approach.  These residual clusters were chosen from each dataset and from different base forecasting approaches.  The images shown here are from a mix of base forecasters without regard to the forecaster's accuracy.  Such clusters have been generated for every combination of dataset and forecaster residuals.

\ref{fig:merl_clusters} is a set of clusters from a residual time series of MERL dataset extracted using the SVM forecaster.  The residuals are from a time series forecasting horizon of 20 minutes (two time steps).  The extracted window length is 70 minutes (seven time steps).  These clusters were taken from the top 10\% of the MERL data residuals.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.00\textwidth]{merl_sample_clusters_svm.png}
	\end{center}
	\caption{Extracted residuals from the MERL dataset.  Data was taken from the top 10\% of the SVM forecaster's residuals.  The red line in the center of the clusters represents the cluster average.}
	\label{fig:merl_clusters}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width = 1.0\textwidth]{brown_sample_clusters_arima.png}
	\end{center}
	\caption{Extracted residuals from the Brown dataset.  Data was taken from the top 10\% of the ARIMA forecaster's residuals.  The red line in the center of the clusters represents the cluster average.}
	\label{fig:brown_clusters}
\end{figure}

\ref{fig:brown_clusters} is the set of clusters from the Brown dataset extracted from the ARIMA forecaster.  The clusters from the dataset were taken from the top 10\% of the Brown data residuals.  Notice how these clusters, unlike the clusters from \ref{fig:merl_clusters} do not begin nor end with a residual value of zero.  This likely means that the true residuals are longer in length than those displayed here.  However, our method of silhouette score maximization returned this set as the best.  Most likely this is due to non-consistent residual behavior if the extracted residual window is increased.
  
\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{denver_sample_clusters_tdnn.png}
	\end{center}
	\caption{Extracted residuals from the Denver dataset.  Data was taken from the top 10\% of the TDNN forecaster's residuals.}
	\label{fig:denver_clusters}
\end{figure}

\ref{fig:denver_clusters} displays the set of modeled clusters from the Denver dataset extracted from the TDNN forecaster.  These clusters were taken from the top 10\% of the forecaster's residuals.  The time series within the Denver dataset correspond to 30 minute intervals.  Thus, the clusters shown in \ref{fig:denver_clusters} demonstrate events that occur over at least 3 hours (six time steps).  Also, some of the clusters begin or end with a 0 residual, but others drift higher or lower.  We believe this demonstrates events which may not have finished during the duration of the clusters demonstrated here.  Ideally we would like to have clusters long enough to fit the entire event, but short enough that they don't contain "normal" behavior.  Clusterings such as this demonstrate a scenario where modeling these clusters through a non-fixed length technique may lead to better results.

Also, notice how some of the clusters above show certain member activities which are somewhat dissimilar from the red cluster line.  While we didn't implement it in this work, it is certainly feasible to extract outlier (or dissimilar) activities from the potential pool of activities and rerun the mixture of Gaussians algorithm.  This sort of outlier extraction would likely lead to slightly better anomaly models.

Finally, we believe that the structure demonstrated from these clusters again leads further credibility to the residual data containing some information about the activity or anomaly that generated the data.  These residual clusters do not appear to be simply the product of random noise.  


\subsection{ABCF derived}
\label{sec:abcf}
Here we present the math behind our Anomaly Bayesian combined forecasting algorithm.  Similar to the Bayesian combined forecaster introduced in Chapter 4, our ABCF algorithm is recursive and computationally efficient.  

We present the algorithm using two anomaly event models $a$ and $b$ with a background model $c$.  Extending the algorithm to work for any number of algorithms is apparent from this presentation.  For our work, the models $a$ and $b$ are modeled anomalies from our time series mixture of Gaussians; however this derivation allows the models to be any of a stochastic time series model such that it is possible to compute $p(x_{t} | a^{(i)}_{t})$.  Finally, the model $c$ is assumed to be a background model which allows for the computation of $p(c_{t}|x_{t})$.  

Let $a_{t}^{(i)} = $ the event that model $a$ is active at time $t$ at index $i$ and $b_{t}^{(i)} = $ the event that model $b$ is active at time $t$ at index $i$.  Both anomalies $a$ and $b$ have a known length defined by the function $len(a)$ or $len(b)$.  For each time $t$ we call the data that generated $t$ as event $e$.  Because $a$ and $b$ are the only anomalies modeled, $e$ can be either ${a^{(i)}_{t}}$, ${b^{(i)}_{t}}$ or $c$ for any $i$.

Thus, we know that

\begin{equation}
p(e_{t}) = 1.
\end{equation}

Since, every $e_{t}$ is generated from either $a$, $b$, or $c$, we force (through normalization)

\begin{equation}
\sum_{i}^{len(a)} p(a_{t}^{(i)}|x_{t}, \ldots, x_{1}) + \sum_{i}^{len(b)} p(b_{t}^{(i)}|x_{t}, \ldots, x_{1}) + p(c_{t}|x_{t}, \ldots, x_{1}) = 1.
\label{eq:pabc}
\end{equation}

The goal of this forecaster is then to find each of $p(a_{t + 1}^{(i + 1)})$, $p(b_{t + 1}^{(i + 1)})$, and $p(c_{t + 1}).$  Given these model likelihoods, we can then produce a final forecast in a way similar to BCF in chapter 4 by either using the most likely model or producing an aggregate forecast from each model.

From Bayes rule we have the following equation 

\begin {equation}
p(a_{t + 1}^{(i + 1)}|x_{t + 1}, \ldots, x_{1}) =
		\frac{p(x_{t + 1}|a^{(i + 1)}_{t + 1}, x_{t}, \ldots, x_{1}) p(a_{t + 1}^{(i + 1)}|x_{t}, \ldots, x_{1})}
	       {p(x_{t + 1})}.
\label{eq:update_bayes}
\end{equation}

To find $p(a_{t + 1}^{(i + 1)}|x_{t + 1}, \ldots, x_{1})$ we need to compute $p(x_{t + 1}|a^{(i + 1)}_{t + 1}, x_{t}, \ldots, x_{1})$,  $p(a_{t + 1}^{(i + 1)}|x_{t}, \ldots, x_{1})$ and $p(x_{t + 1})$.

Computing $p(x_{t + 1})$ is straight forward from the law of total probability

\begin{equation}
	\begin{split}
		p(x_{t + 1}) = \sum_{i}^{len(a) - 1}p(x_{t + 1}|a_{t + 1}^{(i + 1)}, x_{t}, \ldots, x_{1})
						   p(a_{t + 1}^{(i + 1)}|x_{t}, \ldots, x_{1}) + \\
				      \sum_{i}^{len(b) - 1}p(x_{t + 1}|b_{t + 1}^{(i + 1)}, x_{t}, \ldots, x_{1}) 
						   p(b_{t + 1}^{(i + 1)}|x_{t}, \ldots, x_{1}) + \\
	        			      p(x_{t + 1}|c_{t + 1}, x_{t}, \ldots, x_{1})p(c_{t + 1}|x_{t}, \ldots, x_{1}).
	\end{split}
\end{equation}

The computation for $p(x_{t + 1}|a^{(i + 1)}_{t + 1}, x_{t}, \ldots, x_{1})$ is performed directly by computing the likelihood of a new data point $x_{t + 1}$ for model $a$ at offset $i + 1$.

Finally, the calculation of $p(a_{t + 1}^{(i + 1)}|x_{t}, \ldots, x_{1})$ is from an earlier forecast and is what gives our algorithm its recursive nature.  We pass the posterior of all values of $a_{t}$ to the priors of $a_{t + 1}.$  Thus $p(a_{t + 1}^{(i + 1)}|x_{t}, \ldots, x_{1})$ is simply a passed value from the last time step in our algorithm.  Specifically we define 

\begin{equation}
	p(a_{t + 1}^{(i + 1)} | x_{t}, \ldots, x_{1}) = 
		\begin{cases}
			p(a_{t}^{(i)}|x_{t}, \ldots, x_{1}) & : i < len(a) \\
			0 & : otherwise.
		\end{cases}
	%\right
\end{equation}

In the present of an event we pass along the likelihood of that even to the next time step and recompute the posterior of that event.  The prior for the start of each event is a user tunable parameter.  

\begin{equation}
	\begin{split}
		p(a_{t + 1}^{(1)}|x_{t}, \ldots, x_{1}) = p(a_{t + 1}^{(1)}) = \theta_a \\
		p(b_{t + 1}^{(1)}|x_{t}, \ldots, x_{1}) = p(b_{t + 1}^{(1)}) = \theta_b
	\end{split}
\end{equation}

\noindent
Equation \ref{eq:update_bayes} now turns into 

\begin{equation}
p(a_{t + 1}^{(i + 1)}|x_{t + 1}, \ldots, x_{1}) =
		\frac{p(x_{t + 1}|a^{(i + 1)}_{t + 1}, x_{t}, \ldots, x_{1}) p(a_{t + 1}^{(i)}|x_{t}, \ldots, x_{1})}
	       {p(x_{t + 1})}.
\label{eq:update_prior}
\end{equation}

\noindent
The prior is now $p(a_{t + 1}^{(i)}|x_{t}, \ldots, x_{1})$ which does not depend on $x_{t + 1}$ or $a^{i + 1}$.  Performing equation \ref{eq:update_prior} on both $a$ and $b$ and computing $c$ directly yields equation \ref{eq:pabc} which, when normalized equals $1$.  The normalized values of $p(a_{t + 1}^{(i + 1)}$, $p(b_{t + 1}^{i + 1})$, and $p(c)$ are then used to produce either a most likely or weighted aggregate forecast.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Demonstration of ABCF
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!t]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{sample_residual_plot_dataset_Denver.png}
	\end{center}
	\caption{Sample residual and the respective probabilities of each time step for the base forecaster and the a single activity model.  The teal line represents the likelihood of the background ARIMA model.  The tan lines correspond to the likelihood each time step from one of the anomaly clusters.}
	\label{fig:sample_abcf_residual}
\end{figure}

\subsection{Demonstration of ABCF}
\ref{fig:sample_abcf_residual} is a demonstration of ABCF.  This figure displays a sample time frame (in blue) from the Denver dataset along with the corresponding region for one standard deviation of noise from a best fit ARIMA forecaster with a horizon of two time steps.  The teal line below the graph is the likelihood of the background ARIMA model accurately representing the residual data from the set of clustered anomalies.  The remaining tan lines are the individual likelihoods of each time step of a given clustered anomaly shown in \ref{fig:sample_cluster}  

The red line is the resulting ABCF forecast to the residual.  Ideally the residual should be completely zero, thus resulting in no forecasting error.  At approximately time 10, an anomalous event begins to occur.  The ABCF algorithm takes a few time steps to identify this anomaly and determine that the background ARIMA forecaster is no longer accurate.  We see this effect by noticing the teal line's reduction in likelihood from approximately time step 12 to time step 17.  During this time all other extracted clusters are attempting to fit the anomaly.

\begin{wrapfigure}{R}{0.3\textwidth}
\centering
\includegraphics[width=0.25\textwidth]{denver_sample_cluster.png}
\caption{Extracted sample clustered anomaly.}
\label{fig:sample_cluster}
\end{wrapfigure}

The recursive nature of our likelihood time step propagation in the ABCF algorithm is apparent in the tan lines of this sample figure.  At time step 13, there is a slight perturbation in the second tan line.  This perturbation continues to rise both in value (indicating an increase in likelihood that this cluster is active) and time along the anomaly.  Finally at about time step 18, the model is finished.  At this point the likelihood of the background model begins to rise again and eventually the background model (the teal line) is once again the most prevalent model amongst all the extracted anomalies.

\ref{fig:sample_abcf_residual} displays a significant improvement in RMSE-ONAN.  While the early stages of the anomaly show no improvement, the residual error during the middle of the anomaly is roughly halved and the residual error at the end of the anomaly is nearly zero.  \ref{fig:sample_abcf_denver} shows the final result of this forecast window.  The ARIMA forecaster is delayed in its forecast response to the anomaly, but because such an anomaly was commonly seen within this dataset, the ABCF algorithm is able to estimate the anomaly and improve forecasting accuracy during the remainder of the anomaly.

\begin{figure}[!h]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{sample_plot_Denver.png}
	\end{center}
	\caption{Two horizon time step results when applying ABCF to an ARIMA forecaster for a segment of the Denver dataset.}
	\label{fig:sample_abcf_denver}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Results of ABCF
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results of ABCF}
Because the results are given over multiple forecasting horizons with several base forecasters and multiple forecasting metrics, we believe that tables are not an adequate format to display our results.  Also, since the number of potential images to display about the improvements due to ABCF is quite large, we provide a few images here to demonstrate our results and put the remaining images in Appendix \ref{app:results} for the interested reader.  The images presented here first demonstrate our ABCF technique applied to the Denver dataset.  Later images in this section display more aggregated results for each of the datasets, forecasting models and forecasting metrics.

\bigskip
\noindent \textbf{Denver dataset results} 

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=\textwidth]{rmseonan_average_abcf_Denver.png}
	\end{center}
	\caption{RMSE-ONAN vs forecasting horizon for Average model and Average + ABCF on the Denver dataset}
	\label{fig:average_abcf_rmseonan_denver}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=\textwidth]{rmseonan_arima_abcf_Denver.png}
	\end{center}
	\caption{RMSE-ONAN vs forecasting horizon for ARIMA model and ARIMA + ABCF on the Denver dataset}
	\label{fig:arima_abcf_rmseonan_denver}
\end{figure}


\ref{fig:average_abcf_rmseonan_denver} demonstrates the improvements to the RMSE-ONAN metric of applying our ABCF algorithm to the average forecasting algorithm for the Denver dataset.  We found our largest improvements came with the historic average model.  The reason likely being that any activities or consistent deviations from normal behavior show up in the average model.  These behaviors are well represented in the residuals and thus make good candidate models for our ABCF model.


\begin{figure}[p]
	\begin{center}
		\includegraphics[width=\textwidth]{rmseonan_svm_abcf_Denver.png}
	\end{center}
	\caption{RMSE-ONAN vs forecasting horizon for SVM model and SVM + ABCF on the Denver dataset}
	\label{fig:svm_abcf_rmseonan_denver}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=\textwidth]{rmseonan_TDNN_abcf_Denver.png}
	\end{center}
	\caption{RMSE-ONAN vs forecasting horizon for TDNN model and TDNN + ABCF on the Denver dataset}
	\label{fig:tdnn_abcf_rmseonan_denver}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=\textwidth]{rmseonan_abcf_bcf_Denver.png}
	\end{center}
	\caption{RMSE-ONAN vs forecasting horizon for BCF-TS model and BCF-TS + ABCF on the Denver dataset}
	\label{fig:bcf_abcf_rmseonan_denver}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=\textwidth]{rmseonan_improvement_for_each_forecaster_for_Denver.png}
	\end{center}
	\caption{}
	\label{fig:rmseonan_improve_denver}
\end{figure}

\ref{fig:arima_abcf_rmseonan_denver}, \ref{fig:svm_abcf_rmseonan_denver}, and \ref{fig:tdnn_abcf_rmseonan_denver} display the improvement to the RMSE-ONAN metric when applying ABCF to an ARIMA, SVM and TDNN forecaster on the Denver dataset.  While smaller, the improvements are still significant and present out to roughly six time steps.  Interestingly, the TDNN model showed much more mixed results.  Generally applying ABCF resulted in better results, but this was not always the case.  We believe this is due to the significantly more volatile forecasting results of the TDNN model.  From \ref{fig:rmseplotdenver} we know that the TDNN model was generally the worst forecaster for the Denver dataset with substantial variance in forecasting.  Also, Neural Networks are known to suffer a lack of robustness to outliers \cite{connor1994}.  Techniques have been developed to increase outlier robustness, but we did not explore them for this work.

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmse_abcf_svm_arim_tdnn_for_Denver.png}
	\end{center}
	\caption{RMSE value of SVM, ARIMA, and TDNN models with and without ABCF on the Denver dataset}
	\label{fig:rmse_compare_denver_svm}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmse_abcf_average_bcf_bcfts_for_Denver.png}
	\end{center}
	\caption{RMSE value of Average, BCF and BCF-TS models with and without ABCF on the Denver dataset}
	\label{fig:rmse_compare_denver_bcf}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{mase_abcf_svm_arim_tdnn_for_Denver.png}
	\end{center}
	\caption{MASE value of SVM, ARIMA, and TDNN models with and without ABCF on the Denver dataset}
	\label{fig:mase_compare_denver_svm}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{mase_abcf_average_bcf_bcfts_for_Denver.png}
	\end{center}
	\caption{MASE value of Average, BCF and BCF-TS with and without ABCF on the Denver database}
	\label{fig:mase_compare_denver_bcf}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmse_percent_improvement_for_each_forecaster_for_Denver.png}
	\end{center}
	\caption{Percentage improvement using ABCF to RMSE on the Denver dataset for each forecaster}
	\label{fig:rmse_improvement_percent_denver}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{mase_percent_improvement_for_each_forecaster_for_Denver.png}
	\end{center}
	\caption{Percentage improvement using ABCF to MASE on the Denver dataset for each forecaster}
	\label{fig:mase_improvement_percent_denver}
\end{figure}



\ref{fig:bcf_abcf_rmseonan_denver} shows the results of applying ABCF to the BCF-TS model introduced in the previous chapter.  In this ensemble forecaster, the ABCF technique is still able to find enough structured anomalies to reduce the RMSE-ONAN metric.  Despite BCF-TS being an ensemble with TDNN included, the sensitivity to outliers has been reduced and ABCF demonstrates an improvement up to 6 forecasts into the future.  Forecasts of 7 or 8 time steps appear to be slightly worse applying ABCF.  This is not a surprising result as the extracted activities for the Denver dataset were 6 time steps in length.

An aggregate of the prior graphs as a percentage of improvement is shown in \ref{fig:rmseonan_improve_denver}.  In almost all cases, applying ABCF to a base forecaster for the Denver dataset resulted in forecasting improvement to RMSE-ONAN.  The improvements generated from ABCF are highest on short forecasts and drop to roughly zero for forecasts longer than 6 time steps into the future.  For every forecaster we achieve an improvement of at least 20\% at one time step horizon.  This improvement is still 15-20\% for most forecasters at three time steps.  Put in context, the forecasting accuracy of many anomalous traffic events has improved 15 - 20\% over 3 hours into the future.

Aggregated results using RMSE and MASE for application of ABCF for each of our forecasting models on the Denver dataset are given in \ref{fig:rmse_compare_denver_svm}, \ref{fig:rmse_compare_denver_bcf}, \ref{fig:mase_compare_denver_svm}, and \ref{fig:mase_compare_denver_bcf}.  The colors in these figures correspond to the forecasting algorithm used.  The dashed line is the application of ABCF to a given forecasting algorithm.  For most cases, there is little difference in RMSE or MASE performance when applying ABCF.  TDNN shows the biggest difference, but as explained before this is likely due to its sensitivity to outliers.  

Finally, the percent improvement gained by ABCF as measured by RMSE and MASE is shown in \ref{fig:rmse_improvement_percent_denver} and \ref{fig:mase_improvement_percent_denver}.  When compared to \ref{fig:rmseonan_improve_denver} the improvement is much less significant, however in most forecasters a noticeable improvement is still present.  The average forecaster outside of the oddly unpredictable TDNN forecaster tends to show the largest improvement.  The stronger forecasters (SVM, ARIMA and BCF) show little change to both MASE and RMSE values at forecasting horizons greater than three.  This result is to be expected as ABCF only attempts to improve upon anomalous forecasts.  By their very definition, anomalous forecasts represent a relatively small percentage of the overall dataset.  Perhaps most important is that ABCF showed essentially no decrease in forecasting performance in nearly all models and forecasting horizons (again with the exception of TDNN).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Aggregated Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\noindent \textbf{Remaining dataset results} 

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmseonan_improvement_for_each_forecaster_for_Merl.png}
	\end{center}
	\caption{Percentage improvement using ABCF to RMSE-ONAN on the MERL dataset for each forecaster}
	\label{fig:rmseonan_improve_merl}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmseonan_improvement_for_each_forecaster_for_Brown.png}
	\end{center}
	\caption{Percentage improvement using ABCF to RMSE-ONAN on the Brown dataset for each forecaster}
	\label{fig:rmseonan_improve_brown}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmse_percent_improvement_for_each_forecaster_for_Merl.png}
	\end{center}
	\caption{Percentage improvement using ABCF to RMSE on the MERL dataset for each forecaster}
	\label{fig:rmse_improve_merl}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{rmse_percent_improvement_for_each_forecaster_for_Brown.png}
	\end{center}
	\caption{Percentage improvement using ABCF to RMSE on the Brown dataset for each forecaster}
	\label{fig:rmse_improve_brown}
\end{figure}

\ref{fig:rmseonan_improve_merl}, and \ref{fig:rmseonan_improve_brown} show the percentage improvement to the RMSE-ONAN metric using ABCF to supplement a given base forecaster.  The improvement due to ABCF varies greatly based on both forecaster and dataset, however some similarities arose.  SVM and BCF forecasters generally did not improve as much as other forecasters.  TDNN still shows the most fluctuation compared to other forecasters in RMSE-ONAN improvement.

As ABCF is an ensemble forecaster, it can make no guarantees on its improvement.  Despite this lack of guarantee, in nearly all situations on the RMSE-ONAN metric, ABCF demonstrated improved results.  Negative effects on forecasting performance were rare occurring mostly with the TDNN forecaster whose potential deficiencies to our ABCF were described earlier.

\ref{fig:rmse_improve_merl} and \ref{fig:rmse_improve_brown} show the improvement of applying ABCF to each model using RMSE. \ref{fig:mase_improve_merl} and \ref{fig:mase_improve_brown} show the improvements of ABCF using MASE.  These images demonstrate that our forecasting improvements when applying ABCF are typically quite small (<5\%) when considering the entire time series.  These improvements are tied to the density of anomalies within the dataset.  A dataset with more anomalies relative to the forecaster will have the potential to experience a larger overall improvement in forecasting error.

Overall these results demonstrate that ABCF has the power to improve anomaly forecasting while having a little to no detrimental effect on the overall forecasting performance of our forecaster.  Discounting the erratic TDNN forecaster, our worse case scenario occurred when applying ABCF to an SVM forecaster on the MERL dataset and for a forecasting horizon of 2 and 3 we experienced 3 - 4\% error in overall forecasting performance as measured by MASE and RMSE.  However, in almost all other scenarios ABCF improved the overall forecasting accuracy and greatly improved the accuracy of anomaly forecasting as measured by our RMSE-ONAN metric.

Based on these results, for similar datasets, we anticipate no reason why applying ABCF on top of another forecaster wouldn't improve anomaly forecasting with minimal effect on overall forecasting accuracy.  



\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{mase_percent_improvement_for_each_forecaster_for_Merl.png}
	\end{center}
	\caption{Percentage improvement using ABCF to MASE on the MERL dataset for each forecaster}
	\label{fig:mase_improve_merl}
\end{figure}

\begin{figure}[p]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{mase_percent_improvement_for_each_forecaster_for_Brown.png}
	\end{center}
	\caption{Percentage improvement using ABCF to MASE on the Brown dataset for each forecaster}
	\label{fig:mase_improve_brown}
\end{figure}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Future work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Future unexplored work on ABCF}
While we are happy with the current status of the ABCF algorithm, we would recommend the following potential avenues for improvement.

\begin{enumerate}
	\item Explore different data extraction and clustering techniques.  More research into techniques which do not rely on fixed length extracted residual windows.  If a reliance on a sliding window extraction method is to continue, then trimming or growing the sliding window based on the shape of any potential residual.  Another possibility would be to use a sparse dictionary encoding technique to determine the a best dictionary of anomalies for each residual time series.
	
	\item Acquire and test with other datasets.  The datasets presented here give a baseline to test from, but we realize that it does not represent all types of human traffic data.  We would like to test from a traffic dataset with shorter time scales (recall that the Denver dataset is every 30 minutes).  Also we would like to have other types of building data.  Do our approaches work with apartments or government buildings?  Rich building datasets would allow us to answer this question and may lead to improvements in our approach based on the results.
	
	\item Test with additional forecasters.  The four base forecasters we've selected represent a good base, but other forecasters, such as wavelet based or Markov based, may produce some unintuitive results.  We are especially interested in a forecaster or cluster which has a non-Gaussian noise distribution.  Our algorithm allows for any noise distribution, but all of our tests have only been with Gaussian noise.
	
	\item Explore different distributions for ABCF derivation.  Selection of clustered anomalies superficially seems like a topic selection problem \cite{McNamara2007}.  The anomalies could be modeled using a Dirchilet prior instead of a Gaussian prior.  The recursive Bayesian derivation would likely be similar to the current ABCF algorithm. 
\end{enumerate}

